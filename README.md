# cdx-summarize

Use to create a summary for one or more CDX files per year with data about videos, images, html, pdf and http vs https sites. 
```
usage: cdx-summarize.py [-h] [--gz] [--nogz] [--monthly] [file [file ...]]

Summarize CDX file(s) to JSONL, automatically uses gzip filter if file ends
with .gz

positional arguments:
  file        cdx file (can be several)

optional arguments:
  -h, --help  show this help message and exit
  --gz        force use of gzip filter
  --nogz      force not using gzip filter
  --monthly   break up statistics into monthly buckets instead of yearly
```

# combine-summary
This program combines several of these summaries into a single one where each 2nd level domain only appears once. It can also run on a single file where then any duplicate entries for a single 2nd level domain are added together.
```
usage: combine-summary.py [-h] [--yearly] [file [file ...]]

Combine summary files generated by cdx-summary.py

positional arguments:
  file        summary file (can be several)

optional arguments:
  -h, --help  show this help message and exit
  --yearly    force output into yearly buckets
```

# Output file format
The output file format of the tools are files with the following structure:
```
host.tld {"year": {"n_XXX": A, ... "s_XXX":B}}
```
By default only the second-level domain is kept and all other host information is discarded so that information from all hosts in a second-level domain is aggregated together into a single entry. The years are determined by the date in the CDX(J) files. The `n_` fields are counters of the number of entries with a given MIME-type and the `s_` fields are the corresponding sizes of the compressed entries in the WARC files. Only entries with a HTTP status code of 2XX are counted, so redirects and errors etc. are ignored.

# Summarizing the MIME Types

## MIME Type short intro
There are hundreds of valid MIME Types registered with IANA (Internet Assigned Numbers Authority) and the current list can be viewed at [https://www.iana.org/assignments/media-types/media-types.xhtml](https://www.iana.org/assignments/media-types/media-types.xhtml). While this list is extensive, in reality webservers do not always conform to it and return other strings. Webbrowsers are quite leniant and do still handle the files correctly in most cases.

Also, web archives have different levels of information available about MIME types. There are the following:
1. MIME types as specified by the server
2. MIME types as determined by an external utility
For example the unix utility [file](https://linux.die.net/man/1/file) can be run with `file --mime-type` to determine some MIME types. [DROID](https://www.nationalarchives.gov.uk/documents/information-management/droid-user-guide.pdf) can be used to determine MIME types. It depends on each webarchive whether they do the characterisation of the files inside of the WARCs or not.

## Why summarize
The common MIME types are summarised in that they are grouped into several categories. This is mainly to enable the programs to run with less memory requirements (only the number of entries and sizes per category need to be kept). An added benefit is that then it becomes easier to compare the categories later.

## The categories used here:
The categories are specified in the module `mime_counter.py` as follows:
| MIME(s) | category | rationale |
| --- | --- | --- |
| text/html<br/>application/xhtml+xml<br/>text/plain | HTML | These are counted as "web-pages" by [Internet Archive](https://blog.archive.org/2016/10/23/defining-web-pages-web-sites-and-web-captures/) |
| text/css | CSS | interesting for changing usage in formatting pages |
| image/* | IMAGE | all image types are grouped together |
| application/pdf | PDF | Interesting independetly, although IA groups PDFs in "web-page" too |
| video/* | VIDEO | all videos |
| audio/* | AUDIO | all audio types |
| application/javascript<br/>text/javascript<br/>application/x-javascript | JS | these 3 MIME types are common for javascript |
| application/json<br/>text/json | JSON | relatively common and indicates dynamic pages |
| font/*<br/>application/vnd.ms-fontobject<br/>application/font*<br/>application/x-font* | FONT | Usage of custom fonts |

# Data sources used

## Internet Archive metadata summary service
It's possible to get metadata in JSON format from the Internet Archive using this service:

[https://web.archive.org/__wb/search/metadata?q=tld:lu](https://web.archive.org/__wb/search/metadata?q=tld:lu)

Here with the example of the Top-level Domain (TLD) ".lu". There is unfortunately not that much public information available on how exactly these numbers were calculated. The following information is available in the JSON result:
- "captures", per year, per MIME-type, probably the number of resources with status 2XX that were captured
- "new", probably the new domains and hosts captured in the year the metadata was computed
- "new_urls", per year, per MIME-type, probably the number of *new* resources with status 2XX that were captured (according to their SURT notation)
- "timestamp", probably when the summary was last calculated
- "total", per year, probably the total number of 2nd level domains and hosts that returned resources with a 2XX status. 
- "type", the query type, in this case always *tld*
- "urls", per year, per MIME-type, probably the number of resources with status 2XX that were captured and that were unique during that year, according to their SURT notation
- "urls_total_compressed_size", per year, per MIME-type, the size of the compressed WARC records for "urls"

As you can see, there are some unknowns in the data and especially the "total" key seems to be strange, since for the TLD .lu it reports 2285 domains only when the CDX files show otherwise. At the date of writing the `timestamp` is the 22nd of September 2020, so the data for at least 2020 is incomplete.

## CDX files
Using Ilya Kreymer's excellent [cdx-index-client](https://github.com/ikreymer/cdx-index-client), you can download the CDX files from any CDX server that you have access to. 

### Internet Archive CDX server
Using cdx-index-client, you can download the data from the Internet Archive's CDX server which lives at:
(http://web.archive.org/cdx/search/cdx)[http://web.archive.org/cdx/search/cdx]
There is a good description of the capabilities on (Karl-Rainer Blumthal's archive-it blogpost)[https://support.archive-it.org/hc/en-us/articles/115001790023-Access-Archive-It-s-Wayback-index-with-the-CDX-C-API]

### Common-crawl CDX files
Again, using cdx-index-client, you can download the CDXJ indexes from the common-crawl.

### Luxembourg Webarchive CDXJ files
Since I have access to the CDXJ files of the Luxembourg Webarchive, I could run the commands locally.

## Limitations
Some CDXJ files from the commoncrawl do not have MIME types. These are only counted in the `_other` fields. Some dates are also invalid, these lines are ignored.

# Ingesting into elasticsearch
One way of examiming the summary is to ingest them into elasticsearch and then run analytics on them using Kibana.